{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53464390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e693c418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755693346.290084 3275273 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
    "                             timeout=None)\n",
    "\n",
    "\n",
    "# Server\n",
    "# - Handles incoming connections (in this case, via stdio)\n",
    "# - Handles tool discovery (when clients ask \"what tools do you have\")\n",
    "# - Execute tools when requested and returns results\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"/Users/damien/VisualStudio Workspace/AI-Agents/langgraph_mcp_multiserver/assistant/my_mcp/local_servers/math_server.py\"],\n",
    ")\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "\n",
    "        # Initialize the connecion\n",
    "        await session.initialize()\n",
    "\n",
    "        # Convert MCP tools to LangChain tools\n",
    "        tools = await load_mcp_tools(session)\n",
    "\n",
    "        # Create and run the agent\n",
    "        agent = create_react_agent(llm, tools)\n",
    "        agent_response = await agent.ainvoke({\"messages\": \"What's (3 + 5) x 12\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "974d57aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's (3 + 5) x 12\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (10cb3179-efb2-46ea-b969-8eaa4e13b318)\n",
      " Call ID: 10cb3179-efb2-46ea-b969-8eaa4e13b318\n",
      "  Args:\n",
      "    a: 3.0\n",
      "    b: 5.0\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "8\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (23798cd7-edab-4105-8282-b900cff9fcb2)\n",
      " Call ID: 23798cd7-edab-4105-8282-b900cff9fcb2\n",
      "  Args:\n",
      "    a: 8.0\n",
      "    b: 12.0\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "96\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "(3 + 5) x 12 = 96\n"
     ]
    }
   ],
   "source": [
    "for m in agent_response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ab2b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755693313.846131 3275273 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1755693313.855247 3275273 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1755693315.612578 3275273 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1755693316.644469 3275273 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "I0000 00:00:1755693321.332000 3275273 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's the weather in nyc?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_forecast (10454d88-810d-43af-b565-e3064a9b203f)\n",
      " Call ID: 10454d88-810d-43af-b565-e3064a9b203f\n",
      "  Args:\n",
      "    city_name: New York\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_forecast\n",
      "\n",
      "Weather in New York (40.71427, -74.00597):\n",
      "- Temperature: 20.3°C\n",
      "- Wind speed: 13.1 km/h\n",
      "- Weather code: 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The weather in New York is 20.3°C with a wind speed of 13.1 km/h. The weather code is 3.\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "\n",
    "client = MultiServerMCPClient({\n",
    "    \"math\": {\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"/Users/damien/VisualStudio Workspace/AI-Agents/langgraph_mcp_multiserver/assistant/my_mcp/local_servers/math_server.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "    \"weather\": {\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"/Users/damien/VisualStudio Workspace/AI-Agents/langgraph_mcp_multiserver/assistant/my_mcp/local_servers/weather_server.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "})\n",
    "\n",
    "agent = create_react_agent(llm, await client.get_tools())\n",
    "math_response = await agent.ainvoke({\"messages\": \"What's (3 + 5) x 12?\"})\n",
    "weather_response = await agent.ainvoke({\"messages\": \"What's the weather in nyc?\"})\n",
    "\n",
    "for m in weather_response['messages']:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
